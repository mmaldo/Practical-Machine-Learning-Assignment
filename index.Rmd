---
title: "Practical Machine Learning Assignment"
author: "Matthew L Maldonado"
date: "April 30, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predicting Activity Movement


## Load in libraries that will be needed

```{r}
library(caret)
library(e1071)
```

## Read in the data sets

```{r}
train<-read.csv('pml-training.csv')
test<-read.csv('pml-testing.csv')
```


## Exploring the training set and cleaning up the data sets

Let's start by looking at the different columns.

I will begin by eleiminating the variables with little data to use for predicting the class. I will do this to both the testing and the training set to prepare for the upcoming building and testing of the models.These columns inlucde a lot of blanks and "NA" entries. This is shown with the "str" and the "head" command below.

```{r}
head(train)
names(train)
str(train,list.len=999)
```

## Remove the Columns that are not needed

```{r}
tidyTrain<-train[,8:length(names(train))]
tidyTest<-test[,8:length(names(test))]


tidyTrain <- subset(tidyTrain, select = -c(max_roll_belt,max_picth_belt,min_roll_belt,min_pitch_belt,amplitude_roll_belt,amplitude_pitch_belt,
                                           var_total_accel_belt,avg_roll_belt,stddev_roll_belt,var_roll_belt,avg_pitch_belt,stddev_pitch_belt,
                                           var_pitch_belt,avg_yaw_belt,stddev_yaw_belt,var_yaw_belt,var_accel_arm,avg_roll_arm,stddev_roll_arm,var_roll_arm,avg_pitch_arm,
                                           stddev_pitch_arm,var_pitch_arm,avg_yaw_arm,stddev_yaw_arm,var_yaw_arm,max_roll_arm,max_picth_arm,max_yaw_arm,min_roll_arm,
                                           min_pitch_arm,min_yaw_arm,amplitude_roll_arm,amplitude_pitch_arm,amplitude_yaw_arm,max_roll_dumbbell,max_picth_dumbbell,
                                           min_roll_dumbbell,min_pitch_dumbbell,amplitude_roll_dumbbell,amplitude_pitch_dumbbell,var_accel_dumbbell,avg_roll_dumbbell,
                                           stddev_roll_dumbbell,var_roll_dumbbell,avg_pitch_dumbbell,stddev_pitch_dumbbell,var_pitch_dumbbell,avg_yaw_dumbbell,
                                           stddev_yaw_dumbbell,var_yaw_dumbbell,max_roll_forearm,max_picth_forearm,min_roll_forearm,min_pitch_forearm,
                                           amplitude_roll_forearm,amplitude_pitch_forearm,var_accel_forearm,avg_roll_forearm,stddev_roll_forearm,var_roll_forearm,
                                           avg_pitch_forearm,stddev_pitch_forearm,var_pitch_forearm,var_pitch_forearm,avg_yaw_forearm,stddev_yaw_forearm,var_yaw_forearm,
                                           kurtosis_roll_belt,kurtosis_picth_belt,kurtosis_yaw_belt,skewness_roll_belt,skewness_roll_belt.1,skewness_yaw_belt,max_yaw_belt,
                                           min_yaw_belt,amplitude_yaw_belt,kurtosis_roll_dumbbell,kurtosis_picth_dumbbell,kurtosis_yaw_dumbbell,skewness_roll_dumbbell,
                                           skewness_pitch_dumbbell,skewness_yaw_dumbbell,max_yaw_dumbbell,min_yaw_dumbbell,amplitude_yaw_dumbbell,kurtosis_roll_forearm,
                                           kurtosis_picth_forearm,kurtosis_yaw_forearm,skewness_roll_forearm,skewness_pitch_forearm,skewness_yaw_forearm,max_yaw_forearm,
                                           min_yaw_forearm,amplitude_yaw_forearm,kurtosis_roll_arm,kurtosis_picth_arm,kurtosis_yaw_arm,skewness_roll_arm,skewness_pitch_arm,
                                           skewness_yaw_arm))

tidyTest <- subset(tidyTest, select = -c(max_roll_belt,max_picth_belt,min_roll_belt,min_pitch_belt,amplitude_roll_belt,amplitude_pitch_belt,
                                           var_total_accel_belt,avg_roll_belt,stddev_roll_belt,var_roll_belt,avg_pitch_belt,stddev_pitch_belt,
                                           var_pitch_belt,avg_yaw_belt,stddev_yaw_belt,var_yaw_belt,var_accel_arm,avg_roll_arm,stddev_roll_arm,var_roll_arm,avg_pitch_arm,
                                           stddev_pitch_arm,var_pitch_arm,avg_yaw_arm,stddev_yaw_arm,var_yaw_arm,max_roll_arm,max_picth_arm,max_yaw_arm,min_roll_arm,
                                           min_pitch_arm,min_yaw_arm,amplitude_roll_arm,amplitude_pitch_arm,amplitude_yaw_arm,max_roll_dumbbell,max_picth_dumbbell,
                                           min_roll_dumbbell,min_pitch_dumbbell,amplitude_roll_dumbbell,amplitude_pitch_dumbbell,var_accel_dumbbell,avg_roll_dumbbell,
                                           stddev_roll_dumbbell,var_roll_dumbbell,avg_pitch_dumbbell,stddev_pitch_dumbbell,var_pitch_dumbbell,avg_yaw_dumbbell,
                                           stddev_yaw_dumbbell,var_yaw_dumbbell,max_roll_forearm,max_picth_forearm,min_roll_forearm,min_pitch_forearm,
                                           amplitude_roll_forearm,amplitude_pitch_forearm,var_accel_forearm,avg_roll_forearm,stddev_roll_forearm,var_roll_forearm,
                                           avg_pitch_forearm,stddev_pitch_forearm,var_pitch_forearm,var_pitch_forearm,avg_yaw_forearm,stddev_yaw_forearm,var_yaw_forearm,
                                           kurtosis_roll_belt,kurtosis_picth_belt,kurtosis_yaw_belt,skewness_roll_belt,skewness_roll_belt.1,skewness_yaw_belt,max_yaw_belt,
                                           min_yaw_belt,amplitude_yaw_belt,kurtosis_roll_dumbbell,kurtosis_picth_dumbbell,kurtosis_yaw_dumbbell,skewness_roll_dumbbell,
                                           skewness_pitch_dumbbell,skewness_yaw_dumbbell,max_yaw_dumbbell,min_yaw_dumbbell,amplitude_yaw_dumbbell,kurtosis_roll_forearm,
                                           kurtosis_picth_forearm,kurtosis_yaw_forearm,skewness_roll_forearm,skewness_pitch_forearm,skewness_yaw_forearm,max_yaw_forearm,
                                           min_yaw_forearm,amplitude_yaw_forearm,kurtosis_roll_arm,kurtosis_picth_arm,kurtosis_yaw_arm,skewness_roll_arm,skewness_pitch_arm,
                                           skewness_yaw_arm))

```


## Create training and test sets

Next I will break up the testing set even further and create a new training and testing set using cross validation.This is bassically 1-fold cross validation. The smaller K should produce more bias. The previous testing set is then held out for after the model is built and validated.

```{r}
set.seed(353)
inTrain <- createDataPartition(y=tidyTrain$classe, p = 0.7, list = FALSE)
training <- tidyTrain[inTrain,]
testing <- tidyTrain[-inTrain,]
```
 
 We'll first train a Decision Tree, SVM, Boosting using the GBM package, a parallelized Random Forest and a non parallelized Random Forest.
 
```{r,cache=FALSE}
set.seed(353)
## Training a Model
modFitDT <- train(classe~., data = training, method ="rpart")
predDT <- predict(modFitDT,testing)
cmDT <- confusionMatrix(predDT, testing$classe)
cmDT

modFitGBM <- train(classe~., data = training, method ="gbm",verbose = FALSE)
predGBM <- predict(modFitGBM,testing)
cmGBM <- confusionMatrix(predGBM, testing$classe)
cmGBM

modFit <- train(classe~., data = training, method ="parRF", prox = TRUE)
pred <- predict(modFit,testing)
cmRF <- confusionMatrix(pred, testing$classe)
cmRF

modFitRFNonPar <- train(classe~., data = training, method ="rf", prox = TRUE)
pred <- predict(modFit,testing)
cmRF <- confusionMatrix(pred, testing$classe)
cmRF

modelSVM <- svm(classe~ ., data = training)
predSVM <- predict(modelSVM,testing)
cmSVM <- confusionMatrix(predSVM, testing$classe)
cmSVM
```

The decsion tree has rather poor performance so we'll move on to using an SVM,GBM or Random Forest for our final test set. The two Random Forests should have similar performance. The only difference between the parallelized Random Forest and Random Forest model are one uses parallel computation.

Next we'll predict on our validation set which we will submit for grading. I estimate the out of sample error will be in the 90-95% range for the SVM, 99% for the Random Forest,94%-96% for the GBM  and 45% for the Decision Tree. Given the higher out of sample accuracy of the Random Forest model I will use that model for the final predicitons and submission. These estimates were validated with the following predicitons on the final test set.


## SVM Predicitons
```{r}
predict(modelSVM,tidyTest)

```

This would also agree with the result of the quiz which is 19/20 correct.

## Random Forest Predicitons
```{r}
predict(modFitRFNonPar,tidyTest)

```

```{r}
predict(modFit,tidyTest)

```

The Random Forest preformed quite well with 20 out of 20 correct.

## GBM Predicitons

```{r}
predict(modFitGBM,tidyTest)
```

The GBM also ppreformed quite well with 20 out of 20 correct.

## Decision Tree Predicitons

```{r}
predict(modFitDT,tidyTest)

```

The Decision Tree preformed quite poorly with 8 out of 20 correct.

## Conclusion

The Random Forest is the highest preforming model on the data set. Further work can be put into preprocessiong the data and reducing the dimensiaonlity of the data set to see if training time can be improved for the models that take long to train. The fastest training time with an above 90% accuracy was done by the Support Vector Machine and provides a good trade off of training time and accuracy.

